# -*- coding: utf-8 -*-
"""Level1&2_Descriptives_Inferential.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O3hJjh1uhic-wWoFgSqyi_Ho5dny18wK
"""

import pandas as pd
data = pd.read_csv('/content/Heart_Disease.csv')
print(data.head())

data.tail()

data.shape

#Data Types
print(data.dtypes)

#Selection of variables of interest in dataset
print('#Selection of variables of interest in dataset')
print("\n")
data = pd.DataFrame(data, columns = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])
print(data.head())

# To Check for Missing Values in the data
print(data.isnull().sum())

data.describe().T

data.groupby('Sex').describe()

data.groupby('HeartDiseaseorAttack').describe()

data.groupby('Age').describe()

data['HeartDiseaseorAttack'].value_counts()

data['Diabetes_binary'].value_counts()

data['HighBP'].value_counts()

data['HighChol'].value_counts()

data['CholCheck'].value_counts()

data['BMI'].value_counts()

data['Smoker'].value_counts()

data['Stroke'].value_counts()

data['PhysActivity'].value_counts()

data['Fruits'].value_counts()

data['Veggies'].value_counts()

data['HvyAlcoholConsump'].value_counts()

data['Sex'].value_counts()

data['Age'].value_counts()

data['Education'].value_counts()

data['Income'].value_counts()

#Measure of central tendency and disoersion for selected variables
print('Measures of central Tendencies & Dispersion for selected variables')

print("\n")
print("Mean for Age and HeartDiseaseorAttack")
print(data[["Age", "HeartDiseaseorAttack"]].mean())
print("\n")

print("Mean for Sex and HeartDiseaseorAttack")
print(data[["Sex", "HeartDiseaseorAttack"]].mean())
print("\n")
print("Mean for BMI and HeartDiseaseorAttack")
print(data[["BMI", "HeartDiseaseorAttack"]].mean())

print("\n")
print("Mean for Stroke and HeartDiseaseorAttack")
print(data[["Stroke", "HeartDiseaseorAttack"]].mean())
print("\n")

print("Mean for High Cholesterol and HeartDiseaseorAttack")
print(data[["HighChol", "HeartDiseaseorAttack"]].mean())
print("\n")

print("Mean for Diabetes and HeartDiseaseorAttack")
print(data[["Diabetes_binary", "HeartDiseaseorAttack"]].mean())
print("\n")
print("Mean for Smoking and HeartDiseaseorAttack")
print(data[["Smoker", "HeartDiseaseorAttack"]].mean())
print("\n")
print("Mean for AlcoholUse and HeartDiseaseorAttack")
print(data[["HvyAlcoholConsump", "HeartDiseaseorAttack"]].mean())
print("\n")

#Descriptives
print("Descriptives for Variables")
print("\n")
print(data.agg({'Diabetes_binary': ['min','max','mean','median','skew','kurtosis'],
                   'HighBP':['min','max','mean','median','skew','kurtosis'],
                   'HighChol':['min','max','mean','median','skew','kurtosis'],
                   'CholCheck':['min','max','mean','median','skew','kurtosis'],
                   'BMI':['min','max','mean','median','skew','kurtosis'],
                   'Stroke':['min','max','mean','median','skew','kurtosis'],
                   'PhysActivity':['min','max','mean','median','skew','kurtosis'],
                   'Fruits':['min','max','mean','median','skew','kurtosis'],
                   'Veggies':['min','max','mean','median','skew','kurtosis'],
                   'Sex':['min','max','mean','median','skew','kurtosis'],
                   'Age':['min','max','mean','median','skew','kurtosis'],
                   'Education':['min','max','mean','median','skew','kurtosis'],
                   'Income':['min','max','mean','median','skew','kurtosis'],
                   'HeartDiseaseorAttack': ['min','max','mean','median','skew','kurtosis'],
                   'HvyAlcoholConsump':['min','max','mean','median','skew','kurtosis'],
                   'Smoker':['min','max','mean','median','skew','kurtosis']}))

print("Means for the age column for grouped sex and Education (1=No Edu, 2=Early Childhood Edu, 3=Primary Edu, 4=High School, 5=College, 6=Tertiary)")

print(data.groupby(["Sex","Education"])["Age"].mean())

#Frequency Counts of Variables
print("How many male and female have heart disease")
print("\n")
print(data.groupby("Sex")["HeartDiseaseorAttack"].value_counts())

#Frequency Counts of Variables
print("Age in levels have heart disease")
print("\n")
print(data.groupby("Age")["HeartDiseaseorAttack"].value_counts())

print("Age in levels have heart disease")
print("\n")
print(data.groupby("Education")["HeartDiseaseorAttack"].value_counts())

print("Age in levels have heart disease")
print("\n")
print(data.groupby("Income")["HeartDiseaseorAttack"].value_counts())

print("High BP levels have heart disease")
print("\n")
print(data.groupby("HighBP")["HeartDiseaseorAttack"].value_counts())

print("BMI levels have heart disease")
print("\n")
print(data.groupby("BMI")["HeartDiseaseorAttack"].value_counts())

#Frequency Counts of Variables
print("Age in levels have heart disease")
print("\n")
print(data.groupby("Age")["BMI"].value_counts())

data.plot(kind='scatter', x=['BMI'], y= ['Age'])

data.plot(kind='scatter', x=['BMI'], y= ['HeartDiseaseorAttack'])

data['Sex'].value_counts().plot(kind='bar')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns 
import scipy.stats as stats

sns.histplot(x=data['Age'])

import matplotlib.pyplot as plt

# Pie chart, where the slices will be ordered and plotted counter-clockwise:
labels = 'HeartDisease', 'No_HeartDisease'
sizes = [14.78109,85.21891]
explode = (0.1, 0)  # only "explode" the 1st slice (i.e. 'HeartDx = 1')

fig1, ax1 = plt.subplots()
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
shadow=True, startangle=20)
ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

plt.show()

print('Popn Total = 70692')

sns.histplot(x=data['HeartDiseaseorAttack'],
             bins=2, kde=False,
             stat = "probability",
             color="black");

data.groupby('Age')['HeartDiseaseorAttack'].describe()

#Filter Categorical Features
df = data[['HeartDiseaseorAttack', 'Diabetes_binary', 'HighBP', 'HighChol', 'CholCheck', 'Stroke', 'PhysActivity', 'Fruits', 'Veggies', 'Sex', 'Education', 'HvyAlcoholConsump', 'Smoker']]

print(df.head())

import numpy as np
import pandas as pd
import scipy as stat
import seaborn as sns
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df.head().info()
print("\n")
 
print(df.describe())
print("\n")
 
print("Total number of males and females with heart disease in the dataset")
groupby_sex = df.groupby(["Sex", "HeartDiseaseorAttack"]).size()
print(groupby_sex)
print("\n")
 
crosstab = pd.crosstab(df["Sex"], df["HeartDiseaseorAttack"])
print(crosstab)
print("\n")
 
w, x, y, z = stats.chi2_contingency(crosstab)
print("The Chi Square value is:", w)
print("The pvalue is:", x)
print("The value for degree of freedom is :", y)
print("Expected cell counts is:", z)
print("\n")
 
 
#To capture the relevant frequency values and store in variables
print("Total number of males and females with heart disease in the dataset")
a,b,c,d = df.groupby(["Sex", "HeartDiseaseorAttack"]).size()
print("Number of females with no heart disease was:", a)
print("Number of females with heart disease was:", b)
print("Number of males with no heart disease was:", c)
print("Number of males with heart disease was:", d)
 
 
#Repeat the Chi Square Independence Test for the following table
print("\n")
print("Find the frequency count for grouped by sex (female and male), and  Education (1, 2, 3, 4, 5, 6)")
groupby_sex2 = df.groupby(["Sex", "Education"]).size()
crosstab2 = pd.crosstab(df["Sex"], df["Education"])
print(crosstab2)
print("\n")
 
k, l, m, n = stats.chi2_contingency(crosstab2)
print("The Chi Square value is:", k)
print("The pvalue is:", l)
print("The value for degree of freedom is :", m)
print("Expected cell counts is:", n)
print("\n")
 
 
#Repeat the Chi Square Independence Test for the following table
print("\n")
print("Find the frequency count for grouped by Heart disease (0,1), and  Education (1, 2, 3, 4, 5, 6)")
groupby_sex3 = df.groupby(["HeartDiseaseorAttack", "Education"]).size()
crosstab3 = pd.crosstab(df["HeartDiseaseorAttack"], df["Education"])
print(crosstab3)
print("\n")
 
f, g, h, i = stats.chi2_contingency(crosstab3)
print("The Chi Square value is:", f)
print("The pvalue is:", g)
print("The value for degree of freedom is :", h)
print("Expected cell counts is:", i)
print("\n")
 
 
#Repeat the Chi Square Independence Test for the following table
print("\n")
print("Find the frequency count for grouped by sex (male, female), HeartDiseaseorAttack (0,1), and  Education (1, 2, 3, 4, 5, 6)")
crosstab4 = pd.crosstab(df["Sex"], [df["HeartDiseaseorAttack"], df["Education"]])
print(crosstab4)
print("\n")
 
p, q, r, s = stats.chi2_contingency(crosstab4)
print("The Chi Square value is:", p)
print("The pvalue is:", q)
print("The value for degree of freedom is :", r)
print("Expected cell counts is:", s)
print("\n")

#Filter Categorical Features
df = data[['HeartDiseaseorAttack', 'Diabetes_binary', 'HighBP', 'HighChol', 'CholCheck', 'Stroke', 'PhysActivity', 'Fruits', 'Veggies', 'Sex', 'Education', 'HvyAlcoholConsump', 'Smoker']]

print(df.head())

df.describe()

#Measure of Association between the Binary Variables in Dataset using Cram√©r's V
import numpy as np
import pandas as pd
import scipy.stats as stats
import seaborn as sns
from scipy.stats import chi2_contingency
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('/content/Heart_Disease.csv')
df = pd.DataFrame(data, columns = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])
print(data)

df_categorical = data[["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "Smoker", "Stroke", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Education"]]

df_numerical = data[["BMI", "Age", "Income"]]
print ("\n") 

print('Categorical Variables')
print(df_categorical)

print ("\n")
print('Numercal Variables')
print(df_numerical)

df_categorical.shape

df_categorical.head()

df_numerical.shape

df_numerical.head()

#Correlation Heatmaps for Numerical Variables 
 #create a correlation matrix for all the column sets except the target variable

import numpy as np
import pandas as pd
import scipy.stats as stats
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.DataFrame(df_numerical)
#create a correlation matrix for all the column sets except the target variable
correlation = df.corr()
print("\n")
print("Pearson Standard Correlation Coefficient Matrix")
print(correlation)
print("\n")
 
# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(correlation, dtype=bool))
 


 
#Set up matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
 
#Set up a seaborn heatmap
#seaborne aesthetics https://seaborn.pydata.org/tutorial/aesthetics.html 
sns.set_style("white")

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

annot= True
annot_kws = {"size":12}

# Draw the heatmap with the mask and correct aspect ratio
heatmap= sns.heatmap(correlation, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

 
#Add ticks for labels
sns.set_style({'xtick.bottom': True}, {'ytick.left': True})
 
#Export heatmap as an image
heatmap.get_figure().savefig('heatmap_diabetes.png', bbox_inches='tight')

df = pd.DataFrame(data, columns = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])
print(data)

#Correlation Heatmaps for Numerical Variables 
 #create a correlation matrix for all the column sets except the target variable

import numpy as np
import pandas as pd
import scipy.stats as stats
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.DataFrame(data, columns = ["Sex", "Age", "Education", "Income","Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Stroke", "Smoker", "HvyAlcoholConsump", "PhysActivity", "Veggies","Fruits"])

#create a correlation matrix for all the column sets except the target variable
correlation = df.corr()
print("\n")
print("Pearson Standard Correlation Coefficient Matrix")
print(correlation)
print("\n")
 
# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(correlation, dtype=bool))
 
#Set up matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))
 
#Set up a seaborn heatmap
#seaborne aesthetics https://seaborn.pydata.org/tutorial/aesthetics.html 
sns.set_style("white")

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

annot= True
annot_kws = {"size":10}

# Draw the heatmap with the mask and correct aspect ratio
heatmap= sns.heatmap(correlation, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5},
            annot = True, annot_kws = {"size":10})



 
#Add ticks for labels
sns.set_style({'xtick.bottom': True}, {'ytick.left': True})
 
#Export heatmap as an image
heatmap.get_figure().savefig('heatmap_diabetes.png', bbox_inches='tight')

correlation = df.corr()
print("Pearson Standard Correlation Coefficient Matrix")
mask = np.triu(np.ones_like(correlation, dtype=bool))
f, ax = plt.subplots (figsize=(11,9))
sns.set_style("white")
#annot = True
#annot_kws = {"size":12}
heatmap = sns.heatmap(correlation, mask = mask, cmap=cmap, vmax=.3, center= 0,
            square = True, linewidth=.5, cbar_kws={"shrink":.5}),
            #annot = True, annot_kws = {"size":12})
sns.set_style({'xtick.bottom': True}, {'ytick.left': True}),

#create a correlation matrix for all the column sets except the target variable


df = pd.DataFrame(df_numerical)
print (df.head())
 
#Only explore 2 columns: BMI and Income
df1 = df['BMI']
df2 = df['Income']
 
#We would like to explore the relationship between Column 0 (BMI) and Column 2(Income)
pearson_coef_rvalue, p_value = stats.pearsonr(df1, df2) #define the columns to perform calculations on
print("Pearson Correlation Coefficient: ", pearson_coef_rvalue.round(decimals=3), "and a P-value of:", p_value.round(decimals=3)) # Results 
print("\n")
 
#Conduct Correlation Coefficient Hypothesis Testing
#Use 2 tail test
#Confidence level is 95%, alpha is 0.05 and alpha/2 is 0.025
alpha = 0.05
alpha_half = 0.025
 
if p_value < alpha_half:  # null hypothesis: x comes from a normal distribution
    print("Conclusion drawn: The null hypothesis can be rejected")
else:
    print("Conclusion drawn: The null hypothesis is accepted")

#create a correlation matrix for all the column sets except the target variable


df = pd.DataFrame(df_numerical)
print (df.head())
 
#Only explore 2 columns: BMI and Age
df1 = df['Age']
df2 = df['Income']
 
#We would like to explore the relationship between Column 0 (Age) and Column 2(Income)
pearson_coef_rvalue, p_value = stats.pearsonr(df1, df2) #define the columns to perform calculations on
print("Pearson Correlation Coefficient: ", pearson_coef_rvalue.round(decimals=3), "and a P-value of:", p_value.round(decimals=3)) # Results 
print("\n")
 
#Conduct Correlation Coefficient Hypothesis Testing
#Use 2 tail test
#Confidence level is 95%, alpha is 0.05 and alpha/2 is 0.025
alpha = 0.02
alpha_half = 0.025
 
if p_value < alpha_half:  # null hypothesis: x comes from a normal distribution
    print("Conclusion drawn: The null hypothesis can be rejected")
else:
    print("Conclusion drawn: The null hypothesis is accepted")

#Cramer's V Measure of Association
import numpy as np
import pandas as pd
import scipy.stats as ss
import seaborn as sns
from scipy.stats import chi2_contingency
import numpy as np
import matplotlib.pyplot as plt

df_categorical.head()

def cramers_V(var1,var2):
  crosstab =np.array(pd.crosstab(var1,var2, rownames= None, colnames= None))
  stat = chi2_contingency(crosstab)[0]
  obs = np.sum(crosstab)
  mini = min(crosstab.shape)-1
  return (stat/(obs*mini))

df_categoricalHealth = df_categorical[["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "Stroke"]]

df_categoricalSociodemographic = df_categorical[["Sex", "Education"]]

df_categoricalHabit = df_categorical[["Smoker", "HvyAlcoholConsump"]]

df_categoricalLifestyle = df_categorical[["PhysActivity", "Fruits", "Veggies"]]



print("Health Category")
print(df_categoricalHealth)
print("\n")

print("Sociodemographic Category")
print(df_categoricalSociodemographic)
print("\n")

print("Habit Category")
print(df_categoricalHabit)
print("\n")

print("Lifestyle Category")
print(df_categoricalLifestyle)

print(df_categoricalLifestyle.describe())
print("\n")

print("Total number of Study participants who eats and do not eat fruits and veggies in the dataset")
groupby_Fruits = df_categoricalLifestyle.groupby(["Fruits", "Veggies"]).size()
print(groupby_Fruits)
print("\n")
 
crosstab = pd.crosstab(df_categoricalLifestyle["Fruits"], df_categoricalLifestyle["Veggies"])
print(crosstab)
print("\n")
 
#Conduct the Chi Square Independence Test
#Interpretation https://www.pythonfordatascience.org/chi-square-test-of-independence-python/
#The information is returned within a tuple where the first value is the  test static, 
#the second value is the p-value, 
#and the third number is the degrees of freedom. 
#An array is also returned which contains the expected cell counts.
 
w, x, y, z = stats.chi2_contingency(crosstab)
print("The Chi Square value is:", w)
print("The pvalue is:", x)
print("The value for degree of freedom is :", y)
print("Expected cell counts is:", z)
print("\n")
 
 
#To capture the relevant frequency values and store in variables
print("Total number of people who eat fruits and veggies in the dataset")
a,b,c,d = df_categoricalLifestyle.groupby(["Fruits", "Veggies"]).size()
print("Number of people who do not eat fruits and veggies:", a)
print("Number of people who do not eats fruits but veggies:", b)
print("Number of people who eat fruits but now veggies was:", c)
print("Number of people who eats fruits and veggies:", d)
 
 
#Repeat the Chi Square Independence Test for the following table
print("\n")
print("Find the frequency count for grouped by Physical Activity, and  Fruits intake")
groupby_df_categoricalLifestyle = df_categoricalLifestyle.groupby(["PhysActivity", "Fruits"]).size()
crosstab2 = pd.crosstab(df_categoricalLifestyle["PhysActivity"], df_categoricalLifestyle["Fruits"])
print(crosstab2)
print("\n")
 
k, l, m, n = stats.chi2_contingency(crosstab2)
print("The Chi Square value is:", k)
print("The pvalue is:", l)
print("The value for degree of freedom is :", m)
print("Expected cell counts is:", n)
print("\n")
 
 
#Repeat the Chi Square Independence Test for the following table
print("\n")
print("Find the frequency count for grouped by People who engage in physical activites and eat veggies")
groupby_physActivity = df_categoricalLifestyle.groupby(["PhysActivity", "Veggies"]).size()
crosstab3 = pd.crosstab(df_categoricalLifestyle["PhysActivity"], df_categoricalLifestyle["Veggies"])
print(crosstab3)
print("\n")
 
f, g, h, i = stats.chi2_contingency(crosstab3)
print("The Chi Square value is:", f)
print("The pvalue is:", g)
print("The value for degree of freedom is :", h)
print("Expected cell counts is:", i)
print("\n")
 
 
#Repeat the Chi Square Independence Test for the following table
print("\n")
print("Find the frequency count for grouped by Physical Activities, Fruits, and veggies)")
crosstab4 = pd.crosstab(df_categoricalLifestyle["PhysActivity"], [df_categoricalLifestyle["Fruits"], df_categoricalLifestyle["Veggies"]])
print(crosstab4)
print("\n")
 
p, q, r, s = stats.chi2_contingency(crosstab4)
print("The Chi Square value is:", p)
print("The pvalue is:", q)
print("The value for degree of freedom is :", r)
print("Expected cell counts is:", s)
print("\n")

print(df_categoricalHabit.describe())
print("\n")
 
print("Total number of Study participants'Smoking and Alcohol use Habit")
groupby_Smoker = df_categoricalHabit.groupby(["Smoker", "HvyAlcoholConsump"]).size()
print(groupby_Smoker)
print("\n")
 
crosstab = pd.crosstab(df_categoricalHabit["Smoker"], df_categoricalHabit["HvyAlcoholConsump"])
print(crosstab)
print("\n")
 
#Conduct the Chi Square Independence Test
#Interpretation https://www.pythonfordatascience.org/chi-square-test-of-independence-python/
#The information is returned within a tuple where the first value is the  test static, 
#the second value is the p-value, 
#and the third number is the degrees of freedom. 
#An array is also returned which contains the expected cell counts.
 
w, x, y, z = stats.chi2_contingency(crosstab)
print("The Chi Square value is:", w)
print("The pvalue is:", x)
print("The value for degree of freedom is :", y)
print("Expected cell counts is:", z)
print("\n")
 
 
#To capture the relevant frequency values and store in variables
print("Total number participants habit in the dataset")
a,b,c,d = df_categoricalLifestyle.groupby(["Fruits", "Veggies"]).size()
print("Number of people who do not smoke and drink:", a)
print("Number of people who do not smoke but drinks:", b)
print("Number of people who smoke but don't drink:", c)
print("Number of people who eats smokes and drinks:", d)

print(df_categoricalSociodemographic.describe())
print("\n")

print("Total number of males and females with respect to educational status in the dataset")
groupby_sex = df_categoricalSociodemographic.groupby(["Sex", "Education"]).size()
print(groupby_sex)
print("\n")
 
crosstab = pd.crosstab(df_categoricalSociodemographic["Sex"], df_categoricalSociodemographic["Education"])
print(crosstab)
print("\n")
 
#Conduct the Chi Square Independence Test
#Interpretation https://www.pythonfordatascience.org/chi-square-test-of-independence-python/
#The information is returned within a tuple where the first value is the  test static, 
#the second value is the p-value, 
#and the third number is the degrees of freedom. 
#An array is also returned which contains the expected cell counts.
 
w, x, y, z = stats.chi2_contingency(crosstab)
print("The Chi Square value is:", w)
print("The pvalue is:", x)
print("The value for degree of freedom is :", y)
print("Expected cell counts is:", z)
print("\n")
 
 
#To capture the relevant frequency values and store in variables
print("Total number of males and females whether or not educated in the dataset")
a,b,c,d,e,f,g,h,i,j,k,l = df_categoricalSociodemographic.groupby(["Sex", "Education"]).size()
print("Number of females with no education:", a)
print("Number of females with nursery education:", b)
print("Number of females with primary education:", c)
print("Number of females with secondary eduction:", d)
print("Number of females with college eduction:", e)
print("Number of females with tertiary eduction:", f)
print("Number of males with no education:", g)
print("Number of males with nursery education:", h)
print("Number of males with primary education:", i)
print("Number of males with secondary eduction:", j)
print("Number of males with college eduction:", k)
print("Number of males with tertiary eduction:", l)

#Quantile-Quantile (QQ) Plot for Normality using the Age Variable


df = pd.DataFrame(df_numerical, columns = ["BMI"])
print (df.head())

from statsmodels.graphics.gofplots import qqplot
from matplotlib import pyplot as plt


# q-q plot for the age column, line = s is standardised line
qqplot(df, line='s')
plt.show()

#Normalize Data for Computation
from sklearn import preprocessing
import numpy as np

data = pd.DataFrame(data, columns = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])

scaler = preprocessing.MinMaxScaler()
names = data.columns
d = scaler.fit_transform(data)
scaled_df = pd.DataFrame(d, columns=names)
scaled_df.head().T

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
# %matplotlib inline

data = pd.DataFrame(data, columns = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])

scaler = preprocessing.MinMaxScaler()
names = data.columns
d = scaler.fit_transform(data)
scaled_df = pd.DataFrame(d, columns=names)

#Feature selection: split the dataset into features (independent variables) and target (dependent variable)
feature_cols = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"]
X = data[feature_cols] # Features
y = data['HeartDiseaseorAttack'] # Target variable


#PCA to reduce dimensionality in dataset
scaler=StandardScaler()
scaler.fit(data)

scaled_data=scaler.transform(data)
scaled_data
from sklearn.decomposition import PCA
pca=PCA(n_components=2)
pca.fit(scaled_data)
x_pca=pca.transform(scaled_data)
scaled_data.shape

print(x_pca.shape)
print("\n")
print(scaled_data)

print(x_pca)

plt.figure(figsize=(8,6))
plt.scatter(x_pca[:,0],x_pca[:,1],c = y)
plt.xlabel('First principle component')
plt.ylabel('Second principle component')

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import average_precision_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import make_scorer

scaled_df['HeartDiseaseorAttack'].value_counts()

import numpy as np
import pandas as pd
import keras
import seaborn as sns
import matplotlib.pyplot as plt
from time import time
from collections import Counter
from imblearn.over_sampling import SMOTE
plt.figure(figsize = (10, 8))
sns.countplot(scaled_df['HeartDiseaseorAttack'])
plt.show()