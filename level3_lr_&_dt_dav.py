# -*- coding: utf-8 -*-
"""LEVEL3 LR & DT DAV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BwnneiLXaQejZO1NxdlvKOuuMOOA3EfT
"""

import pandas as pd
data = pd.read_csv('/content/Heart_Disease.csv')

data = pd.DataFrame(data, columns = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])

data.head().T

#Normalize Data for Computation
from sklearn import preprocessing
import numpy as np

data = pd.DataFrame(data, columns = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])

scaler = preprocessing.MinMaxScaler()
names = data.columns
d = scaler.fit_transform(data)
scaled_df = pd.DataFrame(d, columns=names)
scaled_df.head().T

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
# %matplotlib inline

data = pd.DataFrame(data, columns = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "HeartDiseaseorAttack", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])

scaler = preprocessing.MinMaxScaler()
names = data.columns
d = scaler.fit_transform(data)
scaled_df = pd.DataFrame(d, columns=names)

#Feature selection: split the dataset into features (independent variables) and target (dependent variable)
feature_cols = ["Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"]
X = data[feature_cols] # Features
y = data['HeartDiseaseorAttack'] # Target variable


#PCA to reduce dimensionality in dataset
scaler=StandardScaler()
scaler.fit(data)

scaled_data=scaler.transform(data)
scaled_data
from sklearn.decomposition import PCA
pca=PCA(n_components=2)
pca.fit(scaled_data)
x_pca=pca.transform(scaled_data)
scaled_data.shape

print(x_pca.shape)
print("\n")
print(scaled_data)

print(x_pca)

plt.figure(figsize=(8,6))
plt.scatter(x_pca[:,0],x_pca[:,1],c = y)
plt.xlabel('First principle component')
plt.ylabel('Second principle component')

data = scaled_df

data.head().T

df = pd.DataFrame(data, columns = ["HeartDiseaseorAttack", "BMI", "Age", "Income", "Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])
df_x = pd.DataFrame(data, columns = ["BMI", "Age", "Income", "Diabetes_binary", "HighBP", "HighChol", "CholCheck", "BMI", "Smoker", "Stroke", "PhysActivity", "Fruits", "Veggies", "HvyAlcoholConsump", "Sex", "Age", "Education", "Income"])


df_y = data[["HeartDiseaseorAttack"]]

print(df.head())
print("\n")
print(df_x.head())
print("\n")
print(df_y.head())

df_y.value_counts().plot.bar()
print("\n")
plt.title("Heart Disease Histogram")
plt.xlabel("Class")
plt.ylabel("Frequency")
df_y.value_counts()

df_x.shape

df_y.shape

# Commented out IPython magic to ensure Python compatibility.
import pandas  as pd
import matplotlib.pyplot as plt
import numpy as np

# %matplotlib inline
from imblearn.over_sampling import SMOTE

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

print("Before OverSampling, counts of label '1': {}".format(sum(y_train==1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train==0)))

sm = SMOTE(random_state=2)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())

print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train_res==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res==0)))

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report

parameters = {
    'C': np.linspace(1, 10, 10)
             }
lr = LogisticRegression()
clf = GridSearchCV(lr, parameters, cv=5, verbose=5, n_jobs=3)
clf.fit(X_train_res, y_train_res.ravel())

clf.best_params_

lr1 = LogisticRegression(C=4,penalty='l2', verbose=5)
lr1.fit(X_train_res, y_train_res.ravel())

import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        #print("Normalized confusion matrix")
    else:
        1#print('Confusion matrix, without normalization')

    #print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

#Confusion Matrix
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)
logreg = LogisticRegression(max_iter=200)
model = logreg.fit(X_train,y_train)
y_pred=model.predict(X_test)

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
print("Matrix: TN[0,0]; FP[0,1]; FN[1,0]; TP[1,1] ")
print(cnf_matrix)

cm = confusion_matrix(y_test, y_pred)
tn = cm[0][0]
fn = cm[1][0]
tp = cm[1][1]
fp = cm[0][1]

print("true negative:", tn)
print("false negative:", fn)
print("true positive:", tp)
print("false positive:", fp)
 
fig, ax = plt.subplots(1,3, figsize=(10,3))
 
#Use seaborn for confusion matrix visualisation
sns.heatmap(cnf_matrix, annot=True, ax=ax[0])
 
 
#seaborn confusion matrix in %
sns.heatmap(cnf_matrix/np.sum(cnf_matrix), annot=True, 
            fmt='.2%', cmap='Blues', ax = ax[1])
 
#heatmap labels
labels = ['True Neg','False Pos','False Neg','True Pos']
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues', ax = ax[2])
 
 
#Model Evaluation
# Model accuracy
print("\n")
print("model score: %.3f" % model.score(X_test, y_test))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))
print("\n")
 
 
#Note if auc value < 0.5 then it is a bad model
auc1 = metrics.roc_auc_score(y_test, y_pred)
print("First AUC value is ", auc1)
print("\n")
 
fpr, tpr, threholds = metrics.roc_curve(y_test, y_pred)
auc2 = metrics.auc(fpr, tpr)
print("Second AUC value is ", auc2)
print("\n")

# Compute ROC curve and ROC area 
 
#y_pred_proba = logreg.predict_proba(X_test)[::,1] 
# predict gives either 0 or 1 as output, while 
# predict_proba - gives the probability of '1'
logreg = LogisticRegression(max_iter = 200) 
model = logreg.fit(X_train,y_train)
y_pred = model.predict(X_test)


y_pred_proba = logreg.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)
auc3 = roc_auc_score(y_test, y_pred_proba)
auc4 = auc(fpr, tpr)
print('fpr, False Positive Rate', fpr)
print('tpr, True Positive Rate', tpr)
print("\n")
print('auc3', auc3)
print("\n")
print('auc4', auc4)
print("\n")
 
plt.figure(figsize=(10,3))
plt.plot(fpr,tpr, label = "auc="+str(auc))
plt.xlabel('false positive rate, fpr')
plt.ylabel('true positive rate, tpr') 
plt.title('ROC Curve')
plt.legend(loc=4)
plt.show()

y_train_pre = lr1.predict(X_train)

cnf_matrix_tra = confusion_matrix(y_train, y_train_pre)

print("Recall metric in the train dataset: {}%".format(100*cnf_matrix_tra[1,1]/(cnf_matrix_tra[1,0]+cnf_matrix_tra[1,1])))


class_names = [0,1]
plt.figure()
plot_confusion_matrix(cnf_matrix_tra , classes=class_names, title='Confusion matrix')
plt.show()

y_pre = lr1.predict(X_test)

cnf_matrix = confusion_matrix(y_test, y_pre)

print("Recall metric in the testing dataset: {}%".format(100*cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])))
#print("Precision metric in the testing dataset: {}%".format(100*cnf_matrix[0,0]/(cnf_matrix[0,0]+cnf_matrix[1,0])))
# Plot non-normalized confusion matrix
class_names = [0,1]
plt.figure()
plot_confusion_matrix(cnf_matrix , classes=class_names, title='Confusion matrix')
plt.show()

tmp = lr1.fit(X_train_res, y_train_res.ravel())

y_pred_sample_score = tmp.decision_function(X_test)


fpr, tpr, thresholds = roc_curve(y_test, y_pred_sample_score)

roc_auc = auc(fpr,tpr)

# Plot ROC
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.0])
plt.ylim([-0.1,1.01])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

roc_auc

# Commented out IPython magic to ensure Python compatibility.
#Decision Tree ALGORITHM
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix

# %matplotlib inline
from imblearn.over_sampling import SMOTE

from sklearn.model_selection import train_test_split



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

print("Before OverSampling, counts of label '1': {}".format(sum(y_train==1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train==0)))

sm = SMOTE(random_state=2)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())

print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train_res==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res==0)))



clf = DecisionTreeClassifier()

clf = clf.fit(X_train, y_train)



y_pred = clf.predict(X_test)

print("\n")

print("Accuracy for 70% training and 30% testing set:",
      metrics.accuracy_score(y_test, y_pred))
print("\n")
print("model score: %.3f" % model.score(X_test, y_test))
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))
print("\n")

print("\n")

print(confusion_matrix(y_test, y_pred))
print("\n")

cm = confusion_matrix(y_test, y_pred)
tn = cm[0][0]
fn = cm[1][0]
tp = cm[1][1]
fp = cm[0][1]

print("true negative:", tn)
print("false negative:", fn)
print("true positive:", tp)
print("false positive:", fp)


y_pred_sample_score = tmp.decision_function(X_test)


fpr, tpr, thresholds = roc_curve(y_test, y_pred_sample_score)

roc_auc = auc(fpr,tpr)

# Plot ROC
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.0])
plt.ylim([-0.1,1.01])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()


roc_auc

from matplotlib import pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree



clf = DecisionTreeClassifier(random_state=1234)
model = clf.fit(X,y)

text_representation = tree.export_text(clf)
print(text_representation)

fig = plt.figure(figsize=(25,20))

tree.plot_tree(clf)

import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        #print("Normalized confusion matrix")
    else:
        1#print('Confusion matrix, without normalization')

    #print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

y_train_pre = clf.predict(X_train)

cnf_matrix_tra = confusion_matrix(y_train, y_train_pre)

print("Recall metric in the train dataset: {}%".format(100*cnf_matrix_tra[1,1]/(cnf_matrix_tra[1,0]+cnf_matrix_tra[1,1])))


class_names = [0,1]
plt.figure()
plot_confusion_matrix(cnf_matrix_tra , classes=class_names, title='Confusion matrix')
plt.show()

y_pre = clf.predict(X_test)

cnf_matrix = confusion_matrix(y_test, y_pre)

#Model Evaluation
# Model accuracy

print("Recall metric in the testing dataset: {}%".format(100*cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])))
#print("Precision metric in the testing dataset: {}%".format(100*cnf_matrix[0,0]/(cnf_matrix[0,0]+cnf_matrix[1,0])))
# Plot non-normalized confusion matrix
class_names = [0,1]
plt.figure()
plot_confusion_matrix(cnf_matrix , classes=class_names, title='Confusion matrix')
plt.show()

y_pred_sample_score = tmp.decision_function(X_test)


fpr, tpr, thresholds = roc_curve(y_test, y_pred_sample_score)

roc_auc = auc(fpr,tpr)

# Plot ROC
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.1,1.0])
plt.ylim([-0.1,1.01])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

roc_auc